18.3

As the training set size increases, the accuracy of the tree generated
from the training set increases. So that means that the correct tree will
always be generated given that the training set size is sufficiently large
enough.

18.4

With decision tree learning, the absolute error is minimized because



18.5

If the ratio (pk / pk + nk) is the same for each attribute split from the 
training set, then the remaining entropy after the split will be the same
as the entropy of the training set, thus resulting in 0 information gain.

18.6

H(Goal) = -((2/5)log2(2/5) + (3/5)log2(3/5)) = 0.971
H(A1) = 0.971 - ((4/5)B(2/4) + (1/5)B(0/1)) = 0.171
H(A2) = 0.971 - ((2/5)B(0/2) + (3/5)B(2/3)) = 0.053
H(A3) = 0.971 - ((2/5)B(1/2) + (3/5)B(1/3)) = 0.02

A1 has largest information gain, so we split on A1

A1 = {x1,x2,x4,x5} and {x3}

H(Goal for A1) = 1
H(A2) = 1 - (2/4)B(2/2) + (2/4)B(0/2) = 1
H(A3) = 1 - (2/4)B(1/2) + (2/4)B(1/2) = 0

A2 has largest information gain, so split at A2 and the tree is finished.


