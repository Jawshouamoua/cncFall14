18.3

As the training set size increases, the accuracy of the tree generated
from the training set increases. So that means that the correct tree will
always be generated given that the training set size is sufficiently large
enough.

18.4

With decision tree learning, the absolute error is minimized because



18.5

If the ratio (pk / pk + nk) is the same for each attribute split from the 
training set, then the remaining entropy after the split will be the same
as the entropy of the training set, thus resulting in 0 information gain.

18.6

H(Goal) = -((2/5)log2(2/5) + (3/5)log2(3/5)) = 0.971
H(A1) = 0.971 - ((4/5)B(2/4) + (1/5)B(0/1)) = 0.171
H(A2) = 0.971 - ((2/5)B(0/2) + (3/5)B(2/3)) = 0.053
H(A3) = 0.971 - ((2/5)B(1/2) + (3/5)B(1/3)) = 0.02

A1 has largest information gain, so we split on A1

A1 = {x1,x2,x4,x5} and {x3}

H(Goal for A1) = 1
H(A2) = 1 - (2/4)B(2/2) + (2/4)B(0/2) = 1
H(A3) = 1 - (2/4)B(1/2) + (2/4)B(1/2) = 0

A2 has largest information gain, so split at A2 and the tree is finished.

18.7

x1--------0--x2----0---- [000,001]
		\		 \_1____ [010,011]
		 \
		  x3-----0------ [100,110]
			 \____1_____ [001,111]

18.11

With LOOCV, only 1/n samples (n=number of instance) is used as the
training set. That means the data is trained on all the data except for
one instance which is used to test the hypothesis. Since the majority 
function only returns the result that is greater than 50% and the data
is trained on 50 positive and negative examples, when the classifier is
used to test, there are 49 instances that have the same result as the test
and 50 instances of the negative example, therefore, the classifier always
fails. 
